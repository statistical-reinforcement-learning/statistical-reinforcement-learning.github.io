<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Exercise 2 • Statistical Reinforcement Learning</title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:slnt,wght@-10..0,100..900&family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="/assets/stylesheets/style.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>
</head>
<body>
  <div class="container">
    <h1>Chapter 1 • Exercise 2</h1>
<h3>Part 1</h3>
Consider the finite class consisting of the two elements \(\mathcal{F} = \{f_0, f_1\}\) where
\begin{align}
f_{i}(x) = i \quad \text{for all $x$}
\end{align}
Now consider the sequnce \((x_i, y_i)\) for \(i = 1, \ldots 2T\) where
\begin{align}
y_{2k-1} = 0, \ y_{2k} = 1 \quad k = 1, \ldots T
\end{align}
For the indicator loss \(l(\cdot, \cdot)\), we must have that
\begin{align}
\min_{f \in \mathcal{F}} \sum_{i=1}^{T} l(f(x_i), y_i) = T
\end{align}
since both \(f_1\) and \(f_2\) achieve the same loss.
Further, one empirical minimizer \(\hat{f^t}\) (since at odd \(t\), \(\hat{f^{t}}\) is not uniquely defined) is given by
\begin{align}
\hat{f}{^{2k-1}} = f_1, \ \hat{f}{^{2k}} = f_0 \quad k = 1, \ldots T 
\end{align}
Hence we have
\begin{align}
\sum_{t=1}^{2T} l(\hat{f}^{t}(x_t), y^t) = 2T
\end{align}
and thus
\begin{align}
\sum_{t=1}^{2T} l(\hat{f}^{t}(x_t), y^t) - \min_{f \in \mathcal{F}} \sum_{i=1}^{T} l(f(x_i), y_i) = T = \Omega(2T)
\end{align}
as desired.
<br><br><em>Note here, the choice of \(\hat{f}^{2k-1}\) is sort of arbitrary. However this doesn't really matter. Let \(p\) be the probability we get the next token right. As long as \(p < 1\), on expectation our regret is still \(\Omega(T)\).</em>

<h3>Part 2</h3>

<em>Big picture: since data is iid, we essentially show that ERM learns \(f^{\star}\)</em>
<br><br>


Let \(\hat{L}^{t}(f) = \frac{1}{t} \sum_{i=1}^{t} l(f(x), y)\) and \(\hat{L}(f) = \hat{L}^{T}(f)\).
<br><br>
Note that since \(f^{t}\) is a function of data \(\mathcal{H}^{t-1}\) and is independent of \((x^t, y^t)\), we have

\begin{align}
  \mathbb{E}\left[\sum_{t=1}^T l(\hat{f}^{t}(x_t), y_t) - T \min \hat{L}(f) \right] &= \sum_{t=1}^T L(\hat{f}^{t}) - T \mathbb{E} \left[ \min_{f \in \mathcal{F}} \hat{L}(f) \right]
\end{align}

If we let \(f^\star = \arg\min L(f)\), then note that for all \(t\), we have by proposition \(1\)

\begin{align}
  L(\hat{f}^{t}) - L(f^\star) &\lesssim \sqrt{\frac{\log|\mathcal{F}|}{t}} \\
 \sum_{t=1}^{T} (L(\hat{f}^{t}) - L(f^\star)) &\lesssim \sqrt{T \log |\mathcal{F}|}
\end{align}

From proof of proposition 1, we know that with probability \(1 - \delta\), for all \(f \in \mathcal{F}\), we have

\begin{align}
L(f) - \hat{L}(f) \leq \sqrt{\frac{1}{2T} \log \frac{|\mathcal{F}|}{\delta}}
\end{align}

By setting \(\hat{f} = \argmin_{f \in \mathcal{F}} \hat{L}(f)\), we have with probability $1 - \delta$

\begin{align}
\hat{L}(\hat{f}) - L(f^\star) &= \hat{L}(\hat{f}) - L(\hat{f}) + L(\hat{f}) - L(f^\star) \\
&\gtrsim - \sqrt{\frac{1}{T} \log \frac{|\mathcal{F}|}{\delta}}
\end{align}

integrating out the tails, we have

\begin{align}
  \mathbb{E}[\hat{L}(\hat{f})] - L(f^{\star}) &\gtrsim - \sqrt{\frac{1}{T} \log \frac{|\mathcal{F}|}{\delta}}
\end{align}

Combining gives

\begin{align}
  \sum_{t=1}^{T} (L(\hat{f}^{t}) - L(f^\star)) - T \left(\mathbb{E}(\hat{L}(\hat{f})) - L(f^\star)\right) &\lesssim \sqrt{|T| \log \mathcal{F}}
\end{align}

as desired.
    <br><br><a href="/">Return home</a>
  </div>
</body>
</html>